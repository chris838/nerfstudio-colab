{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you start\n",
        "\n",
        "- You'll need a Google Drive account to connect to.\n",
        "- Ensure you've selected the `GPU` runtime type in Colab, under `Runtime->Change runtime type->Hardware acceleration`"
      ],
      "metadata": {
        "id": "PbmlQ93kz2bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "Google Drive is typically the easiest way to persist files across notebooks and sessions."
      ],
      "metadata": {
        "id": "Srqm_FQpx3Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (will prompt for permission)\n",
        "%cd /content/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "5a4PDttCx3fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9baea258-5cea-4e9c-c660-bebaae535def"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build wheelhouse\n",
        "\n",
        "Setup is painfully slow for Colab notebooks, these scripts try to pre-build wheels to speed things up.\n",
        "\n",
        "You should be able to build once and re-use. If you find you're getting errors with `tiny-cuda-nn`, you might need to rebuild - although this should be rare (e.g. if Colab updates their python runtime).\n",
        "\n",
        "You can either use the nerfstudio version we build here, or build your own from source in your own notebook - so long as the dependencies remain the same."
      ],
      "metadata": {
        "id": "clgOvGo5kSoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Create wheelhouse that we'll populate with pre-built wheels\n",
        "!mkdir -p /content/gdrive/MyDrive/colab/wheelhouse"
      ],
      "metadata": {
        "id": "56YRnlvPRHGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c76cdc5-3f42-44e0-f3ce-c6540c57a822"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "Successfully installed pip-23.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the python bindings from tiny-cuda-nn and copy to wheelhouse\n",
        "%cd /content/\n",
        "!git clone --recursive https://github.com/NVlabs/tiny-cuda-nn\n",
        "%cd tiny-cuda-nn/bindings/torch\n",
        "!python setup.py bdist_wheel\n",
        "%cp dist/*.whl /content/gdrive/MyDrive/colab/wheelhouse/\n",
        "\n",
        "# Test installing tinycudann from wheelhouse\n",
        "%cd /content/\n",
        "!pip install --no-index --find-links=/content/gdrive/MyDrive/colab/wheelhouse tinycudann\n"
      ],
      "metadata": {
        "id": "dk0L64M-avzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1e6871-b7c8-4cd9-b3a0-f451067dbc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'tiny-cuda-nn'...\n",
            "remote: Enumerating objects: 3773, done.\u001b[K\n",
            "remote: Counting objects: 100% (319/319), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 3773 (delta 172), reused 248 (delta 150), pack-reused 3454\u001b[K\n",
            "Receiving objects: 100% (3773/3773), 19.65 MiB | 40.99 MiB/s, done.\n",
            "Resolving deltas: 100% (2364/2364), done.\n",
            "Submodule 'dependencies/cutlass' (https://github.com/NVIDIA/cutlass) registered for path 'dependencies/cutlass'\n",
            "Submodule 'dependencies/fmt' (https://github.com/fmtlib/fmt) registered for path 'dependencies/fmt'\n",
            "Cloning into '/content/tiny-cuda-nn/dependencies/cutlass'...\n",
            "remote: Enumerating objects: 19806, done.        \n",
            "remote: Counting objects: 100% (2454/2454), done.        \n",
            "remote: Compressing objects: 100% (991/991), done.        \n",
            "remote: Total 19806 (delta 1589), reused 2209 (delta 1427), pack-reused 17352        \n",
            "Receiving objects: 100% (19806/19806), 30.97 MiB | 24.95 MiB/s, done.\n",
            "Resolving deltas: 100% (14288/14288), done.\n",
            "Cloning into '/content/tiny-cuda-nn/dependencies/fmt'...\n",
            "remote: Enumerating objects: 31630, done.        \n",
            "remote: Counting objects: 100% (1540/1540), done.        \n",
            "remote: Compressing objects: 100% (85/85), done.        \n",
            "remote: Total 31630 (delta 1480), reused 1460 (delta 1452), pack-reused 30090        \n",
            "Receiving objects: 100% (31630/31630), 13.68 MiB | 11.09 MiB/s, done.\n",
            "Resolving deltas: 100% (21451/21451), done.\n",
            "Submodule path 'dependencies/cutlass': checked out '1eb6355182a5124639ce9d3ff165732a94ed9a70'\n",
            "Submodule path 'dependencies/fmt': checked out 'b0c8263cb26ea178d3a5df1b984e1a61ef578950'\n",
            "/content/tiny-cuda-nn/bindings/torch\n",
            "Building PyTorch extension for tiny-cuda-nn version 1.7\n",
            "Obtained compute capability 75 from PyTorch\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "Detected CUDA version 11.8\n",
            "running bdist_wheel\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "running build\n",
            "running build_py\n",
            "Generating grammar tables from /usr/lib/python3.9/lib2to3/Grammar.txt\n",
            "Generating grammar tables from /usr/lib/python3.9/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.9\n",
            "creating build/lib.linux-x86_64-3.9/tinycudann\n",
            "copying tinycudann/modules.py -> build/lib.linux-x86_64-3.9/tinycudann\n",
            "copying tinycudann/__init__.py -> build/lib.linux-x86_64-3.9/tinycudann\n",
            "running egg_info\n",
            "creating tinycudann.egg-info\n",
            "writing tinycudann.egg-info/PKG-INFO\n",
            "writing dependency_links to tinycudann.egg-info/dependency_links.txt\n",
            "writing top-level names to tinycudann.egg-info/top_level.txt\n",
            "writing manifest file 'tinycudann.egg-info/SOURCES.txt'\n",
            "reading manifest file 'tinycudann.egg-info/SOURCES.txt'\n",
            "writing manifest file 'tinycudann.egg-info/SOURCES.txt'\n",
            "copying tinycudann/bindings.cpp -> build/lib.linux-x86_64-3.9/tinycudann\n",
            "running build_ext\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.8) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:397: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'tinycudann_bindings._75_C' extension\n",
            "creating dependencies\n",
            "creating dependencies/fmt\n",
            "creating dependencies/fmt/src\n",
            "creating src\n",
            "creating build/temp.linux-x86_64-3.9\n",
            "creating build/temp.linux-x86_64-3.9/tinycudann\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/tiny-cuda-nn/include -I/content/tiny-cuda-nn/dependencies -I/content/tiny-cuda-nn/dependencies/cutlass/include -I/content/tiny-cuda-nn/dependencies/cutlass/tools/util/include -I/content/tiny-cuda-nn/dependencies/fmt/include -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c ../../dependencies/fmt/src/format.cc -o build/temp.linux-x86_64-3.9/../../dependencies/fmt/src/format.o -std=c++14 -DTCNN_MIN_GPU_ARCH=75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_75_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/tiny-cuda-nn/include -I/content/tiny-cuda-nn/dependencies -I/content/tiny-cuda-nn/dependencies/cutlass/include -I/content/tiny-cuda-nn/dependencies/cutlass/tools/util/include -I/content/tiny-cuda-nn/dependencies/fmt/include -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c ../../dependencies/fmt/src/os.cc -o build/temp.linux-x86_64-3.9/../../dependencies/fmt/src/os.o -std=c++14 -DTCNN_MIN_GPU_ARCH=75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_75_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "/usr/local/cuda/bin/nvcc -I/content/tiny-cuda-nn/include -I/content/tiny-cuda-nn/dependencies -I/content/tiny-cuda-nn/dependencies/cutlass/include -I/content/tiny-cuda-nn/dependencies/cutlass/tools/util/include -I/content/tiny-cuda-nn/dependencies/fmt/include -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c ../../src/common.cu -o build/temp.linux-x86_64-3.9/../../src/common.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -std=c++14 --extended-lambda --expt-relaxed-constexpr -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -Xcompiler=-Wno-float-conversion -Xcompiler=-fno-strict-aliasing -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -DTCNN_MIN_GPU_ARCH=75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_75_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -I/content/tiny-cuda-nn/include -I/content/tiny-cuda-nn/dependencies -I/content/tiny-cuda-nn/dependencies/cutlass/include -I/content/tiny-cuda-nn/dependencies/cutlass/tools/util/include -I/content/tiny-cuda-nn/dependencies/fmt/include -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c ../../src/common_device.cu -o build/temp.linux-x86_64-3.9/../../src/common_device.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -std=c++14 --extended-lambda --expt-relaxed-constexpr -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -Xcompiler=-Wno-float-conversion -Xcompiler=-fno-strict-aliasing -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -DTCNN_MIN_GPU_ARCH=75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_75_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -I/content/tiny-cuda-nn/include -I/content/tiny-cuda-nn/dependencies -I/content/tiny-cuda-nn/dependencies/cutlass/include -I/content/tiny-cuda-nn/dependencies/cutlass/tools/util/include -I/content/tiny-cuda-nn/dependencies/fmt/include -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c ../../src/cpp_api.cu -o build/temp.linux-x86_64-3.9/../../src/cpp_api.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -std=c++14 --extended-lambda --expt-relaxed-constexpr -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -Xcompiler=-Wno-float-conversion -Xcompiler=-fno-strict-aliasing -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -DTCNN_MIN_GPU_ARCH=75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_75_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -I/content/tiny-cuda-nn/include -I/content/tiny-cuda-nn/dependencies -I/content/tiny-cuda-nn/dependencies/cutlass/include -I/content/tiny-cuda-nn/dependencies/cutlass/tools/util/include -I/content/tiny-cuda-nn/dependencies/fmt/include -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c ../../src/cutlass_mlp.cu -o build/temp.linux-x86_64-3.9/../../src/cutlass_mlp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -std=c++14 --extended-lambda --expt-relaxed-constexpr -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -Xcompiler=-Wno-float-conversion -Xcompiler=-fno-strict-aliasing -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -DTCNN_MIN_GPU_ARCH=75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=_75_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/tiny-cuda-nn/dependencies/fmt/include/fmt/core.h(288)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1675-D: unrecognized GCC pragma\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build nerfstudio into wheelhouse\n",
        "# Substitute a different branch/commit, if required\n",
        "%cd /content/\n",
        "!git clone -b nerfplayer-testing --single-branch https://github.com/chris838/nerfstudio.git\n",
        "%cd nerfstudio\n",
        "!pip wheel . --wheel-dir /content/gdrive/MyDrive/colab/wheelhouse\n",
        "\n",
        "# Test installing nerfstudio from wheelhouse\n",
        "%cd /content/\n",
        "!pip install --no-index --find-links=/content/gdrive/MyDrive/colab/wheelhouse nerfstudio"
      ],
      "metadata": {
        "id": "bj94XcqBRPQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build COLMAP\n",
        "\n",
        "You can skip this step if you're not using COLMAP, e.g. you already have pre-processed images with camera poses, or you're using Polycam output.\n",
        "\n",
        "If you want to train nerfstudio from raw images or video, you will need COLMAP installed.\n",
        "\n",
        "Building COLMAP takes a while. We build here once and store to Google Drive, running it later is then very fast. \n",
        "\n",
        "By building ourselves, we can use colmap wihtout installing `conda` - saving more time and avoiding a kernel restart."
      ],
      "metadata": {
        "id": "zS6r_CJpOFHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "!sudo apt-get install \\\n",
        "    git \\\n",
        "    cmake \\\n",
        "    ninja-build \\\n",
        "    build-essential \\\n",
        "    libboost-program-options-dev \\\n",
        "    libboost-filesystem-dev \\\n",
        "    libboost-graph-dev \\\n",
        "    libboost-system-dev \\\n",
        "    libboost-test-dev \\\n",
        "    libeigen3-dev \\\n",
        "    libflann-dev \\\n",
        "    libfreeimage-dev \\\n",
        "    libmetis-dev \\\n",
        "    libgoogle-glog-dev \\\n",
        "    libgflags-dev \\\n",
        "    libsqlite3-dev \\\n",
        "    libglew-dev \\\n",
        "    qtbase5-dev \\\n",
        "    libqt5opengl5-dev \\\n",
        "    libcgal-dev \\\n",
        "    libceres-dev\n",
        "  \n",
        "# Build\n",
        "%cd /content/\n",
        "!git clone https://github.com/colmap/colmap\n",
        "%cd colmap\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake .. -GNinja -DCMAKE_CUDA_ARCHITECTURES='70'\n",
        "!ninja\n",
        "!sudo ninja install\n",
        "\n",
        "# Backup build outputs to GDrive\n",
        "%cd /content/\n",
        "%mkdir -p /content/gdrive/MyDrive/colab/colmap/local/bin\n",
        "%mkdir -p /content/gdrive/MyDrive/colab/colmap/local/lib\n",
        "%mkdir -p /content/gdrive/MyDrive/colab/colmap/local/share/applications\n",
        "%mkdir -p /content/gdrive/MyDrive/colab/colmap/local/include\n",
        "\n",
        "%cp /usr/local/bin/colmap /content/gdrive/MyDrive/colab/colmap/local/bin/\n",
        "%cp -r /usr/local/lib/colmap /content/gdrive/MyDrive/colab/colmap/local/lib/\n",
        "%cp /usr/local/share/applications/COLMAP.desktop /content/gdrive/MyDrive/colab/colmap/local/share/applications/\n",
        "%cp -r /usr/local/share/colmap/ /content/gdrive/MyDrive/colab/colmap/local/share/\n",
        "%cp -r /usr/local/include/colmap /content/gdrive/MyDrive/colab/colmap/local/include/\n",
        "\n",
        "%cd /content/gdrive/MyDrive/colab/colmap\n",
        "!zip -r local.zip ./local && rm -rf ./local"
      ],
      "metadata": {
        "id": "4A-bgqxHVIbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [OPTIONAL] Test nerfstudio install from Google Drive\n",
        "\n",
        "Redundant if you're running through this end-to-end, but useful for debugging."
      ],
      "metadata": {
        "id": "AcmRsTnExvxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!pip install --no-index --find-links=/content/gdrive/MyDrive/colab/wheelhouse nerfstudio tinycudann"
      ],
      "metadata": {
        "id": "3XbbRZzXxu1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy test data to Google Drive\n",
        "\n",
        "Copy test datasets to Google Drive, which we can then test in the main notebook.\n",
        "\n",
        "This may seem superfluous, but it does provide a blueprint for putting your own data in Google Drive and accessing it from Colab notebooks."
      ],
      "metadata": {
        "id": "bDOY7WnwrOzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data dir\n",
        "%cd /content/\n",
        "%mkdir /content/gdrive/MyDrive/colab/data"
      ],
      "metadata": {
        "id": "b7Pvyd7vsg6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy nerf_synthetic dataset\n",
        "%cd /content/\n",
        "!ns-download-data blender --save-dir /content/gdrive/MyDrive/colab/data"
      ],
      "metadata": {
        "id": "BeyxMm40sTl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dynamic nerf dataset\n",
        "!ns-download-data dnerf --save-dir /content/gdrive/MyDrive/colab/data"
      ],
      "metadata": {
        "id": "hzguMMn2ta7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}